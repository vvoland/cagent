<h1>Local Models (Ollama, vLLM, LocalAI)</h1>
<p class="subtitle">Run cagent with locally hosted models for privacy, offline use, or cost savings.</p>

<h2>Overview</h2>

<p>cagent can connect to any OpenAI-compatible local model server. This guide covers the most popular options:</p>

<ul>
  <li><strong>Ollama</strong> ‚Äî Easy-to-use local model runner</li>
  <li><strong>vLLM</strong> ‚Äî High-performance inference server</li>
  <li><strong>LocalAI</strong> ‚Äî OpenAI-compatible API for various backends</li>
</ul>

<div class="callout callout-tip">
  <div class="callout-title">üí° Docker Model Runner</div>
  <p>For the easiest local model experience, consider <a href="#providers/dmr" onclick="event.preventDefault(); navigate('providers/dmr')">Docker Model Runner</a> which is built into Docker Desktop and requires no additional setup.</p>
</div>

<h2>Ollama</h2>

<p>Ollama is a popular tool for running LLMs locally. cagent includes a built-in <code>ollama</code> alias for easy configuration.</p>

<h3>Setup</h3>

<ol>
  <li>Install Ollama from <a href="https://ollama.ai/" target="_blank" rel="noopener noreferrer">ollama.ai</a></li>
  <li>Pull a model:
    <pre><code class="language-bash">ollama pull llama3.2
ollama pull qwen2.5-coder</code></pre>
  </li>
  <li>Start the Ollama server (usually runs automatically):
    <pre><code class="language-bash">ollama serve</code></pre>
  </li>
</ol>

<h3>Configuration</h3>

<p>Use the built-in <code>ollama</code> alias:</p>

<pre><code class="language-yaml">agents:
  root:
    model: ollama/llama3.2
    description: Local assistant
    instruction: You are a helpful assistant.</code></pre>

<p>The <code>ollama</code> alias automatically uses:</p>
<ul>
  <li><strong>Base URL:</strong> <code>http://localhost:11434/v1</code></li>
  <li><strong>API Type:</strong> OpenAI-compatible</li>
  <li><strong>No API key required</strong></li>
</ul>

<h3>Custom Port or Host</h3>

<p>If Ollama runs on a different host or port:</p>

<pre><code class="language-yaml">models:
  my_ollama:
    provider: ollama
    model: llama3.2
    base_url: http://192.168.1.100:11434/v1

agents:
  root:
    model: my_ollama
    description: Remote Ollama assistant
    instruction: You are a helpful assistant.</code></pre>

<h3>Popular Ollama Models</h3>

<table>
  <thead><tr><th>Model</th><th>Size</th><th>Best For</th></tr></thead>
  <tbody>
    <tr><td><code>llama3.2</code></td><td>3B</td><td>General purpose, fast</td></tr>
    <tr><td><code>llama3.1</code></td><td>8B</td><td>Better reasoning</td></tr>
    <tr><td><code>qwen2.5-coder</code></td><td>7B</td><td>Code generation</td></tr>
    <tr><td><code>mistral</code></td><td>7B</td><td>General purpose</td></tr>
    <tr><td><code>codellama</code></td><td>7B</td><td>Code tasks</td></tr>
    <tr><td><code>deepseek-coder</code></td><td>6.7B</td><td>Code generation</td></tr>
  </tbody>
</table>

<h2>vLLM</h2>

<p>vLLM is a high-performance inference server optimized for throughput.</p>

<h3>Setup</h3>

<pre><code class="language-bash"># Install vLLM
pip install vllm

# Start the server
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-3.2-3B-Instruct \
  --port 8000</code></pre>

<h3>Configuration</h3>

<pre><code class="language-yaml">providers:
  vllm:
    api_type: openai_chatcompletions
    base_url: http://localhost:8000/v1

agents:
  root:
    model: vllm/meta-llama/Llama-3.2-3B-Instruct
    description: vLLM-powered assistant
    instruction: You are a helpful assistant.</code></pre>

<h2>LocalAI</h2>

<p>LocalAI provides an OpenAI-compatible API that works with various backends.</p>

<h3>Setup</h3>

<pre><code class="language-bash"># Run with Docker
docker run -p 8080:8080 --name local-ai \
  -v ./models:/models \
  localai/localai:latest-cpu</code></pre>

<h3>Configuration</h3>

<pre><code class="language-yaml">providers:
  localai:
    api_type: openai_chatcompletions
    base_url: http://localhost:8080/v1

agents:
  root:
    model: localai/gpt4all-j
    description: LocalAI assistant
    instruction: You are a helpful assistant.</code></pre>

<h2>Generic Custom Provider</h2>

<p>For any OpenAI-compatible server:</p>

<pre><code class="language-yaml">providers:
  my_server:
    api_type: openai_chatcompletions
    base_url: http://localhost:8000/v1
    # token_key: MY_API_KEY  # if auth required

agents:
  root:
    model: my_server/model-name
    description: Custom server assistant
    instruction: You are a helpful assistant.</code></pre>

<h2>Performance Tips</h2>

<div class="callout callout-info">
  <div class="callout-title">‚ÑπÔ∏è Local Model Considerations</div>
  <ul>
    <li><strong>Memory:</strong> Larger models need more RAM/VRAM. A 7B model typically needs 8-16GB RAM.</li>
    <li><strong>GPU:</strong> GPU acceleration dramatically improves speed. Check your server's GPU support.</li>
    <li><strong>Context length:</strong> Local models often have smaller context windows than cloud models.</li>
    <li><strong>Tool calling:</strong> Not all local models support function/tool calling. Test your model's capabilities.</li>
  </ul>
</div>

<h2>Example: Offline Development Agent</h2>

<pre><code class="language-yaml">agents:
  developer:
    model: ollama/qwen2.5-coder
    description: Offline code assistant
    instruction: |
      You are a software developer working offline.
      Focus on code quality and clear explanations.
    max_iterations: 20
    toolsets:
      - type: filesystem
      - type: shell
      - type: think
      - type: todo</code></pre>

<h2>Troubleshooting</h2>

<h3>Connection Refused</h3>
<p>Ensure your model server is running and accessible:</p>
<pre><code class="language-bash">curl http://localhost:11434/v1/models  # Ollama
curl http://localhost:8000/v1/models   # vLLM</code></pre>

<h3>Model Not Found</h3>
<p>Verify the model is downloaded/available:</p>
<pre><code class="language-bash">ollama list  # List available Ollama models</code></pre>

<h3>Slow Responses</h3>
<ul>
  <li>Check if GPU acceleration is enabled</li>
  <li>Try a smaller model</li>
  <li>Reduce <code>max_tokens</code> in your config</li>
</ul>
